{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating a playing cards dataset\n",
    "This notebook is a guide through the creation of a dataset of playing cards. The cards are labeled with their name (ex: \"2c\" for \"2 of spades\", \"Kh\" for King for hearts) and with the bounding boxes delimiting their printed corners.\n",
    "> _Why bounding boxes around the corners, and not around the whole card ?_<br>Because in real conditions, more often than not, cards are partially covered. And the corner of a card is the minimum information you need to identify it.\n",
    "\n",
    "This dataset can be used for the training of a neural net intended to detect/localize playing cards. It was used on the project __[Playing card detection with YOLO v3](https://youtu.be/pnntrewH0xg)__\n",
    "\n",
    "<img src=\"img/ex_generated_image.png\" alt=\"Example of generated image \"  title=\"Example of generated image \" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "\n",
    "### A. In addition to opencv and numpy, you need the following python packages :\n",
    "1. **imgaug** : https://github.com/aleju/imgaug \n",
    "> Helps with image augmentation\n",
    "2. **shapely** : https://github.com/Toblerity/Shapely\n",
    "> For the manipulation and analysis of geometric objects in the Cartesian plane. It is useful here when we want to check if the bounding box of a card corner is covered by another card\n",
    "3. **tqdm** : https://github.com/tqdm/tqdm\n",
    "> A progress bar tool. Not mandatory but convenient when you generate thousands of images\n",
    "\n",
    "### B. Get the Describable Textures Dataset (DTD)\n",
    "> A collection of textural images in the wild (https://www.robots.ox.ac.uk/~vgg/data/dtd/). It is probably not its original goal, but it is used here as an easy way to generate various backgrounds for our images.\n",
    "\n",
    "### C. A real deck of cards\n",
    "> This the only physical \"data\" you need. You have to make some measurements on the cards, and to take a picture/shoot a movie of each of the 52 cards of the deck. Everything else will be generated by scripts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.1 Measurements on the cards\n",
    "Use a ruler to measure the dimensions as indicated in the image below. \n",
    "For the corner* measures, the idea is to delimit one rectangular zone that can hold every value+suit. The size of the marks may vary with the value or the suit, so take the measures on the cards with the widest, the tallest symbols, and add one or two millimeters. \n",
    "<img src=\"img/measures.png\" alt=\"Measures\" title=\"\" />\n",
    "Report the measures in mm in the cell below (and don't forget to run the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardW=60\n",
    "cardH=114\n",
    "cornerXmin=3\n",
    "cornerXmax=9\n",
    "cornerYmin=2\n",
    "cornerYmax=20\n",
    "\n",
    "# We convert the measures from mm to pixels: multiply by an arbitrary factor 'zoom'\n",
    "# You shouldn't need to change this\n",
    "zoom=4\n",
    "cardW*=zoom\n",
    "cardH*=zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### C.2 Shoot one video per card\n",
    "That's what I did in the project \"Playing card detection with YOLO v3\", and the scripts in this notebook were written to deal with videos. A video is interesting only if you can vary the brightness and color temperature of the light during the shot. Watch [Playing card detection with YOLO v3](https://www.youtube.com/watch?v=pnntrewH0xg&t=4m10s) to have an idea. \n",
    "With hindsight, I am confident that you could use one picture per card instead of a video without affecting the efficiency of the neural net. The scripts could be easily adapted to deal with pictures.\n",
    "Whether you use videos or pictures, the scene must be simple, with a uniform background behind the card, like below. This way, the extraction of the card from the scene will be easy.\n",
    "<img src=\"test/scene.png\" alt=\"Scene\" title=\"Scene\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import pickle\n",
    "from glob import glob \n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from shapely.geometry import Polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some convenient functions used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_img(img,polygons=[],channels=\"bgr\",size=9):\n",
    "    \"\"\"\n",
    "        Function to display an inline image, and draw optional polygons (bounding boxes, convex hulls) on it.\n",
    "        Use the param 'channels' to specify the order of the channels (\"bgr\" for an image coming from OpenCV world)\n",
    "    \"\"\"\n",
    "    if not isinstance(polygons,list):\n",
    "        polygons=[polygons]    \n",
    "    if channels==\"bgr\": # bgr (cv2 image)\n",
    "        nb_channels=img.shape[2]\n",
    "        if nb_channels==4:\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)    \n",
    "    fig,ax=plt.subplots(figsize=(size,size))\n",
    "    ax.set_facecolor((0,0,0))\n",
    "    ax.imshow(img)\n",
    "    for polygon in polygons:\n",
    "        # An polygon has either shape (n,2), \n",
    "        # either (n,1,2) if it is a cv2 contour (like convex hull).\n",
    "        # In the latter case, reshape in (n,2)\n",
    "        if len(polygon.shape)==3:\n",
    "            polygon=polygon.reshape(-1,2)\n",
    "        patch=patches.Polygon(polygon,linewidth=1,edgecolor='g',facecolor='none')\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "def give_me_filename(dirname, suffixes, prefix=\"\"):\n",
    "    \"\"\"\n",
    "        Function that returns a filename or a list of filenames in directory 'dirname'\n",
    "        that does not exist yet. If 'suffixes' is a list, one filename per suffix in 'suffixes':\n",
    "        filename = dirname + \"/\" + prefix + random number + \".\" + suffix\n",
    "        Same random number for all the file name\n",
    "        Ex: \n",
    "        > give_me_filename(\"dir\",\"jpg\", prefix=\"prefix\")\n",
    "        'dir/prefix408290659.jpg'\n",
    "        > give_me_filename(\"dir\",[\"jpg\",\"xml\"])\n",
    "        ['dir/877739594.jpg', 'dir/877739594.xml']        \n",
    "    \"\"\"\n",
    "    if not isinstance(suffixes, list):\n",
    "        suffixes=[suffixes]\n",
    "    \n",
    "    suffixes=[p if p[0]=='.' else '.'+p for p in suffixes]\n",
    "          \n",
    "    while True:\n",
    "        bname=\"%09d\"%random.randint(0,999999999)\n",
    "        fnames=[]\n",
    "        for suffix in suffixes:\n",
    "            fname=os.path.join(dirname,prefix+bname+suffix)\n",
    "            if not os.path.isfile(fname):\n",
    "                fnames.append(fname)\n",
    "                \n",
    "        if len(fnames) == len(suffixes): break\n",
    "    \n",
    "    if len(fnames)==1:\n",
    "        return fnames[0]\n",
    "    else:\n",
    "        return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"data\" # Directory that will contain all kinds of data (the data we download and the data we generate)\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "card_suits=['c']\n",
    "card_values=['1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "for card,list_ads\n",
    "\n",
    "# imgW,imgH: dimensions of the generated dataset images \n",
    "imgW=720\n",
    "imgH=720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of the cards from pictures or video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the alphamask\n",
    "The alphamask has 2 purposes:\n",
    "- clean the border of the detected cards,\n",
    "- make that border transparent. Cards are not perfect rectangles because corners are rounded. We need to make transparent the zone between the real card and its bounding rectangle, otherwise this zone will be visible in the final generated images of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bord_size=3 # bord_size alpha=0\n",
    "alphamask=np.ones((cardH,cardW),dtype=np.uint8)*255\n",
    "cv2.rectangle(alphamask,(0,0),(cardW-1,cardH-1),0,bord_size)\n",
    "cv2.line(alphamask,(bord_size*3,0),(0,bord_size*3),0,bord_size)\n",
    "cv2.line(alphamask,(cardW-bord_size*3,0),(cardW,bord_size*3),0,bord_size)\n",
    "cv2.line(alphamask,(0,cardH-bord_size*3),(bord_size*3,cardH),0,bord_size)\n",
    "cv2.line(alphamask,(cardW-bord_size*3,cardH),(cardW,cardH-bord_size*3),0,bord_size)\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(alphamask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function extract_card \n",
    "Extract from scene image (cv2/bgr) the part corresponding to the card and transforms it \n",
    "to fit into the reference card shape.\n",
    "We suppose here that the user facilitates as much as he can the extraction task by\n",
    "making the scene image simple (one card on uniform backgroung, not too blurry, correct lighting,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varianceOfLaplacian(img):\n",
    "    \"\"\"\n",
    "    Compute the Laplacian of the image and then return the focus\n",
    "    measure, which is simply the variance of the Laplacian\n",
    "    Source: A.Rosebrock, https://www.pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\n",
    "    \"\"\"\n",
    "    return cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "def extract_card (img, output_fn=None, min_focus=120, debug=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    imgwarp=None\n",
    "    \n",
    "    # Check the image is not too blurry\n",
    "    focus=varianceOfLaplacian(img)\n",
    "    if focus < min_focus: \n",
    "        if debug: print(\"Focus too low :\", focus)\n",
    "        return False,None\n",
    "    \n",
    "    # Convert in gray color\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Noise-reducing and edge-preserving filter\n",
    "    gray=cv2.bilateralFilter(gray,11,17,17)\n",
    "    \n",
    "    # Edge extraction\n",
    "    edge=cv2.Canny(gray,30,200)\n",
    "    \n",
    "    # Find the contours in the edged image\n",
    "    _,cnts, _ = cv2.findContours(edge.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # We suppose that the contour with largest area corresponds to the contour delimiting the card\n",
    "    cnt = sorted(cnts, key = cv2.contourArea, reverse = True)[0]\n",
    "    \n",
    "    # We want to check that 'cnt' is the contour of a rectangular shape\n",
    "    # First, determine 'box', the minimum area bounding rectangle of 'cnt'\n",
    "    # Then compare area of 'cnt' and area of 'box'\n",
    "    # Both areas sould be very close\n",
    "    rect=cv2.minAreaRect(cnt)\n",
    "    box=cv2.boxPoints(rect)\n",
    "    box=np.int0(box)\n",
    "    areaCnt=cv2.contourArea(cnt)\n",
    "    areaBox=cv2.contourArea(box)\n",
    "    valid=areaCnt/areaBox>0.95\n",
    "    \n",
    "    if valid:\n",
    "        # We want transform the zone inside the contour into the reference rectangle of dimensions (cardW,cardH)\n",
    "        ((xr,yr),(wr,hr),thetar)=rect\n",
    "        # Determine 'Mp' the transformation that transforms 'box' into the reference rectangle\n",
    "        if wr>hr:\n",
    "            Mp=cv2.getPerspectiveTransform(np.float32(box),refCard)\n",
    "        else:\n",
    "            Mp=cv2.getPerspectiveTransform(np.float32(box),refCardRot)\n",
    "        # Determine the warped image by applying the transformation to the image\n",
    "        imgwarp=cv2.warpPerspective(img,Mp,(cardW,cardH))\n",
    "        # Add alpha layer\n",
    "        imgwarp=cv2.cvtColor(imgwarp,cv2.COLOR_BGR2BGRA)\n",
    "        \n",
    "        # Shape of 'cnt' is (n,1,2), type=int with n = number of points\n",
    "        # We reshape into (1,n,2), type=float32, before feeding to perspectiveTransform\n",
    "        cnta=cnt.reshape(1,-1,2).astype(np.float32)\n",
    "        # Apply the transformation 'Mp' to the contour\n",
    "        cntwarp=cv2.perspectiveTransform(cnta,Mp)\n",
    "        cntwarp=cntwarp.astype(np.int)\n",
    "        \n",
    "        # We build the alpha channel so that we have transparency on the\n",
    "        # external border of the card\n",
    "        # First, initialize alpha channel fully transparent\n",
    "        alphachannel=np.zeros(imgwarp.shape[:2],dtype=np.uint8)\n",
    "        # Then fill in the contour to make opaque this zone of the card \n",
    "        cv2.drawContours(alphachannel,cntwarp,0,255,-1)\n",
    "        \n",
    "        # Apply the alphamask onto the alpha channel to clean it\n",
    "        alphachannel=cv2.bitwise_and(alphachannel,alphamask)\n",
    "        \n",
    "        # Add the alphachannel to the warped image\n",
    "        imgwarp[:,:,3]=alphachannel\n",
    "        \n",
    "        # Save the image to file\n",
    "        if output_fn is not None:\n",
    "            cv2.imwrite(output_fn,imgwarp)\n",
    "        \n",
    "    if debug:\n",
    "        cv2.imshow(\"Gray\",gray)\n",
    "        cv2.imshow(\"Canny\",edge)\n",
    "        edge_bgr=cv2.cvtColor(edge,cv2.COLOR_GRAY2BGR)\n",
    "        cv2.drawContours(edge_bgr,[box],0,(0,0,255),3)\n",
    "        cv2.drawContours(edge_bgr,[cnt],0,(0,255,0),-1)\n",
    "        cv2.imshow(\"Contour with biggest area\",edge_bgr)\n",
    "        if valid:\n",
    "            cv2.imshow(\"Alphachannel\",alphachannel)\n",
    "            cv2.imshow(\"Extracted card\",imgwarp)\n",
    "\n",
    "    return valid,imgwarp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function extract_cards_from_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cards_from_video(video_fn, output_dir=None, keep_ratio=3, min_focus=120, debug=False):\n",
    "    \"\"\"\n",
    "        Extract cards from media file 'video_fn' \n",
    "        If 'output_dir' is specified, the cards are saved in 'output_dir'.\n",
    "        One file per card with a random file name\n",
    "        Because 2 consecutives frames are probably very similar, we don't use every frame of the video, \n",
    "        but only one every 'keep_ratio' frames\n",
    "        \n",
    "        Returns list of extracted images\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(video_fn):\n",
    "        print(f\"Video file {video_fn} does not exist !!!\")\n",
    "        return -1,[]\n",
    "    if output_dir is not None and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    cap=cv2.VideoCapture(video_fn)\n",
    "    a=cv2.VideoCapture(video_fn)\n",
    "    a.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "    #print(a.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    #print(a.get(cv2.CAP_PROP_POS_MSEC))\n",
    "    \n",
    "    frame_nb=0\n",
    "    imgs_list=[]\n",
    "    while True:\n",
    "        ret,img=cap.read()\n",
    "        if not ret: break\n",
    "        # Work on every 'keep_ratio' frames\n",
    "        if frame_nb%keep_ratio==0:\n",
    "            if output_dir is not None:\n",
    "                output_fn=give_me_filename(output_dir,\"png\")\n",
    "            else:\n",
    "                output_fn=None\n",
    "            valid,card_img = extract_card(img,output_fn,min_focus=min_focus,debug=debug)\n",
    "            if debug: \n",
    "                k=cv2.waitKey(1)\n",
    "                if k==27: break\n",
    "            if valid:\n",
    "                imgs_list.append(card_img)\n",
    "        frame_nb+=1\n",
    "    \n",
    "    if debug:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return imgs_list\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Card extraction from all the videos\n",
    "We suppose we have for each card_name (ex: 2d, Kc, Ah) one video file named 'card_name.extension' (ex: 2d.avi, Kc.avi, Ah.avi) in a common directory (ex: data/video). If you use images instead of movies, the script below should work by setting the variable 'extension' below to \"jpg\" or \"png\". \n",
    "The cards from a video, or the card from an image, will be extracted in a subdirectory named 'card_name' placed in the directory 'imgs_dir' (ex: data/cards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_cards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f9887066946c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#for suit in card_suits:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#    for value in card_values:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcard_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_cards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m      \u001b[0;31m#   card_name=value+suit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvideo_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcard_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_cards' is not defined"
     ]
    }
   ],
   "source": [
    "video_dir=\"data/video_ver4\"\n",
    "extension=\"mp4\"\n",
    "imgs_dir=\"data/cards3\"\n",
    "\n",
    "#for suit in card_suits:\n",
    "#    for value in card_values:\n",
    "for card_name in list_cards:        \n",
    "     #   card_name=value+suit\n",
    "        video_fn=os.path.join(video_dir,card_name+\".\"+extension)\n",
    "        output_dir=os.path.join(imgs_dir,card_name)\n",
    "        if not os.path.isdir(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        imgs=extract_cards_from_video(video_fn,output_dir)\n",
    "        print(\"Extracted images for %s : %d\"%(card_name,len(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb images for 1c : 15\n",
      "Nb images for 2c : 15\n",
      "Nb images for 3c : 15\n",
      "Nb images for 4c : 15\n",
      "Nb images for 5c : 15\n",
      "Nb images for 6c : 15\n",
      "Nb images for 7c : 15\n",
      "Nb images for 8c : 15\n",
      "Nb images for 9c : 15\n",
      "Saved in : data/cards3.pck\n"
     ]
    }
   ],
   "source": [
    "imgs_dir=\"data/cards3\"\n",
    "\n",
    "num_images_per_card = 15\n",
    "import random\n",
    "cards={}\n",
    "for suit in card_suits:\n",
    "    for value in card_values:\n",
    "        card_name=value+suit        \n",
    "        card_dir=os.path.join(imgs_dir,card_name)\n",
    "        if not os.path.isdir(card_dir):\n",
    "            print(f\"!!! {card_dir} does not exist !!!\")\n",
    "            continue\n",
    "        cards[card_name]=[]\n",
    "        for f in random.sample(glob(card_dir+\"/*.png\"), num_images_per_card):\n",
    "        #for f in glob(card_dir+\"/*.png\"):\n",
    "            img=cv2.imread(f,cv2.IMREAD_UNCHANGED)\n",
    "            # We store the image in \"rgb\" format (we don't need opencv anymore)\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGRA2RGBA)\n",
    "          #  print(img.shape)\n",
    "            cards[card_name].append((img,1,1))\n",
    "        print(f\"Nb images for {card_name} : {len(cards[card_name])}\")\n",
    "\n",
    "        \n",
    "cards_pck_fn=data_dir+\"/cards3.pck\"\n",
    "print(\"Saved in :\",cards_pck_fn)\n",
    "pickle.dump(cards,open(cards_pck_fn,'wb'))\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb images for 1c : 5\n",
      "Nb images for 2c : 5\n",
      "Nb images for 3c : 5\n",
      "Nb images for 4c : 5\n",
      "Nb images for 5c : 5\n",
      "Nb images for 6c : 5\n",
      "Nb images for 7c : 5\n",
      "Nb images for 8c : 5\n",
      "Nb images for 9c : 5\n",
      "Saved in : data/cards3.pck\n"
     ]
    }
   ],
   "source": [
    "imgs_dir=\"data/cards3\"\n",
    "num = 5\n",
    "import random\n",
    "cards={}\n",
    "for suit in card_suits:\n",
    "    for value in card_values:\n",
    "        card_name=value+suit        \n",
    "        card_dir=os.path.join(imgs_dir,card_name)\n",
    "        if not os.path.isdir(card_dir):\n",
    "            print(f\"!!! {card_dir} does not exist !!!\")\n",
    "            continue\n",
    "        cards[card_name]=[]\n",
    "        for f in random.sample(glob(card_dir+\"/*.png\"),num):\n",
    "            img=cv2.imread(f,cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # We store the image in \"rgb\" format (we don't need opencv anymore)\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGRA2RGBA)\n",
    "            cards[card_name].append((img,1,1))\n",
    "        print(f\"Nb images for {card_name} : {len(cards[card_name])}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Saved in :\",cards_pck_fn)\n",
    "pickle.dump(cards,open(cards_pck_fn,'wb'))\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
